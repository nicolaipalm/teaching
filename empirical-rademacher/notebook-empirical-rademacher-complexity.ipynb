{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0e16d38",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e9332d",
   "metadata": {},
   "source": [
    "In this notebook you will find ...\n",
    "1. A recap of the main result of the Rademacher complexity in the context of statistical learning theory\n",
    "2. A playground for applying and testing those (theoretical) results to the hypothesis class of ridge regression, \n",
    "\n",
    "The purpose of this notebook is to empirically validate all the theoretical results as well as getting a feeling how the Rademacher complexity behaves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee0e525",
   "metadata": {},
   "source": [
    "## Recap of mathematical results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09257c5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T11:37:30.888434Z",
     "start_time": "2023-05-31T11:37:30.874287Z"
    }
   },
   "source": [
    "The main result about the Rademacher complexity reads as follows:\n",
    "\n",
    "$$R(\\hat{h})\\le R(h^*)+2\\mathcal{R}_n(\\mathcal{L})+K\\left(\\frac{ln(1/\\delta)}{2n}\\right)^{1/2}$$\n",
    "\n",
    "with probability at least $1-\\delta$ where $R$ denotes the risk, $\\hat{h}=\\mathcal{A}(D_n)$ is the parameter obtained by (our) algorithm, $R(h^*)$ is the minimal risk among all hypothesis, $\\mathcal{R}_n(\\mathcal{L})$ denotes the Rademacher complexity with respect to the composition of loss function with the underlying hypothesis class, $K$ is the supremum of the loss function restricted to the image of the hypothesis class. \n",
    "\n",
    "Furthermore, the empirical Rademacher complexity defined as \n",
    "$$\\hat{\\mathcal{R}}_n(\\mathcal{L})=\\mathbb{E}_{\\epsilon}\\left[sup_{h\\in \\mathcal{H}}\\frac{1}{n}\\sum_{i=1,...,n}\\epsilon_iL\\circ h(Z_i)\\right]$$\n",
    "satisfies \n",
    "$\\mathbb{E}\\left[\\hat{\\mathcal{R}}_n(\\mathcal{L})\\right]=\\mathcal{R}_n(\\mathcal{L})$ and serves, therefore, as an approximate of the Rademacher complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3160baf1",
   "metadata": {},
   "source": [
    "## Import of required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b529f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from modules.parametric_model import PenalizedLinearModel\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f13680",
   "metadata": {},
   "source": [
    "## Generating training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0df863e",
   "metadata": {},
   "source": [
    "We first generate some training data (i.e. sample the random variables $Z$) by defining\n",
    "$$Z=(X,f(X)+\\epsilon)$$\n",
    "where $X$ is uniformly distributed on the interval $[0,1]$, $\\epsilon\\sim \\mathcal{U}[-1/2,1/2]$ is a uniformly distributed noise (independent of $X$) and $f$ is the function defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b29671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_beta = 4\n",
    "def linear_function(x):\n",
    "    return true_beta*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a48870",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# training data (X,f(X)) with X uniform on [0,1]\n",
    "def generate_training_data(number_samples: int):\n",
    "    return [np.array([x, linear_function(x)+np.random.uniform(-1/2,1/2)]) for x in np.random.uniform(0, 1, number_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03640dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 10\n",
    "training_data = generate_training_data(sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70446e4e",
   "metadata": {},
   "source": [
    "## Hypothesis class: Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6c8771",
   "metadata": {},
   "source": [
    "In this notebook we deal with Ridge regression, i.e. our hypothesis class looks as follows:\n",
    "$$\\mathcal{H}=\\{x\\mapsto \\beta^Tx\\colon ||\\beta||_q\\le M\\}$$\n",
    "for some real number $M$ and positive number $q$.\n",
    "\n",
    "In the following we set \n",
    "$$q=2,M=3$$\n",
    "but you are free to change the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bc7b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_of_q_norm = 2\n",
    "M = 3\n",
    "# Initialize ridge regression model\n",
    "model = PenalizedLinearModel(\n",
    "    training_data=training_data, maximum_beta=M, q_of_q_norm=q_of_q_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a288c424",
   "metadata": {},
   "source": [
    "We use the ERM (expected risk minimizer) algorithm in order to obtain a hypothesis from our samples.\n",
    "This is already implemented in the train method of the above model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bca3861",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(initial_guess=np.array([0]), max_iter_training=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43525a5",
   "metadata": {},
   "source": [
    "Let us visualize the training data and the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d327b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model\n",
    "x = np.array(training_data).T[0]\n",
    "y = np.array(training_data).T[1]\n",
    "\n",
    "fig, ax = plt.subplots(layout='constrained')\n",
    "ax.scatter(x, y, label=\"Training data\")\n",
    "ax.plot([0,1], [model(parameter=model.trained_parameter, x=xi)\n",
    "        for xi in [0,1]], label=f\"Trained function; $\\\\beta={model.trained_parameter[0]:.2f}$\")\n",
    "ax.plot([0,1], [linear_function(xi)\n",
    "        for xi in [0,1]], label=f\"True function; $\\\\beta={true_beta:.2f}$\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f531539f",
   "metadata": {},
   "source": [
    "## Empirical Rademacher complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f182c",
   "metadata": {},
   "source": [
    "We wish to calculate the empirical Rademacher complexity \n",
    "$$\\hat{\\mathcal{R}}_n(\\mathcal{L})=\\mathbb{E}_{\\epsilon}(sup_{h\\in \\mathcal{H}}\\frac{1}{n}\\sum_{i=1,...,n}\\epsilon_iL\\circ h(Z_i)).$$\n",
    "However, since we are taking an expected value, we *approximate* the empirical Rademacher complexity by the following algorithm\n",
    "\n",
    "```\n",
    "    for $k=1,...,K$ \n",
    "    1. simulate Rademacher variables \n",
    "```\n",
    "$$\\epsilon_1,...,\\epsilon_n$$\n",
    "```\n",
    "    2. compute \n",
    "```\n",
    "$$\\mathcal{R}_k= max_{h\\in \\mathcal{H}}\\frac{1}{n}\\sum_{i=1}^n\\epsilon_iL\\circ h(Z_i)$$\n",
    "\n",
    "Since $$\\frac{1}{K}\\sum_{k=1}^K\\mathcal{R}_k\\overset{K\\to \\infty}\\to \\hat{\\mathcal{R}}_n$$\n",
    "almost surely, $\\frac{1}{K}\\sum_{k=1}^K\\mathcal{R}_k$ serves as an approximation of the empirical Rademacher complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ffa3a",
   "metadata": {},
   "source": [
    "**Exercise:** Implement the empirical Rademacher complexity according to the above algorithm.\n",
    "\n",
    "**Note:** The loss function (sqare loss) is alread implemented in the model (method: model.loss_function()) and takes two inputs $z_1, z_2$ where $z=(z_1,z_2)=(x,y)$ (i.e. as the form of the training data). \n",
    "\n",
    "\n",
    "For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a7de48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.loss_function(training_data[0], training_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Bonus Exercise:** Implement the empirical Rademacher complexity in the function below. The function should return the value as a float."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b79e29766af0efa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadc24b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_empirical_rademacher_complexity(K: int,\n",
    "#                                              hypothesis_class: PenalizedLinearModel,\n",
    "#                                              max_iter_maximization: float = 1000) -> float:\n",
    "#    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from modules.empirical_rademacher import calculate_empirical_rademacher_complexity  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a70c80cd335d3680"
  },
  {
   "cell_type": "markdown",
   "id": "72be6e2b",
   "metadata": {},
   "source": [
    "Let us calculate the approximation of the empirical Rademacher complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830a419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 100\n",
    "print(\n",
    "    f\"The approximated empirical rademacher complexity with K={K} is {calculate_empirical_rademacher_complexity(K=K,hypothesis_class=model,)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89e51c5",
   "metadata": {},
   "source": [
    "### Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9d38b2",
   "metadata": {},
   "source": [
    "Recall that the Rademacher complexity serves as a bound \n",
    "$$\\sup_{h\\in \\mathcal{H}}(R(h)-R_n(h))\\le 2\\mathcal{R}_n(\\mathcal{L})+K\\sqrt{\\frac{ln(1/\\delta)}{2n}}$$\n",
    "with probability at least $1-\\delta$.\n",
    "\n",
    "Let us verify this empirically by visualizing $R(h)-R_n(h)$ and the upper right side for all $h$.\n",
    "\n",
    "We calculate\n",
    "$$K=sup_{z,l}|l(z)|=sup_{\\beta\\in [-3,3]}(\\beta_{true}-\\beta+1/2)^2=(15/2)^2.$$\n",
    "\n",
    "In our setup we can explicitely calculate the risk (since we know the underlying distribution of the samples):\n",
    "$$R(h)=\\int_{0}^1(4x-\\beta x)^2dx+var(\\epsilon)=\\frac{(4-\\beta)^2}{3}+1/3$$\n",
    "\n",
    "Observe that in our setup $||\\beta||_q\\le M$ reads $\\beta\\in [-3,3]$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c4391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_risk_of_hypothesis(beta):\n",
    "    return ((true_beta-beta)**2)/3+1/3"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In particular, the bound yields \n",
    "$$R(h)\\le R_n(h)+2\\mathcal{R}_n(\\mathcal{L})+K\\sqrt{\\frac{ln(1/\\delta)}{2n}}$$\n",
    "for every $h\\in \\mathcal{H}$ with probability at least $1-\\delta$.\n",
    "In other words, based on the empirical risk $R_n(h)$ (which we can calculate) we can bound the risk $R(h)$ (which we _cannot_ calculate) of every hypothesis $h\\in \\mathcal{H}$. Let us visualize that!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1192183ce495af06"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "x = np.arange(-M, M, 0.1)\n",
    "y = calculate_risk_of_hypothesis(\n",
    "    x)\n",
    "training_data = generate_training_data(sample_size)\n",
    "model = PenalizedLinearModel(\n",
    "    training_data=training_data, maximum_beta=M, q_of_q_norm=q_of_q_norm)\n",
    "model.train(initial_guess=0)\n",
    "delta = 0.05 # 95% confidence\n",
    "rademacher = calculate_empirical_rademacher_complexity(\n",
    "    K=100, hypothesis_class=model)\n",
    "sup_L = (np.max([true_beta-M,true_beta+M])+1/2)**2\n",
    "bound = 2*rademacher+sup_L*np.sqrt(np.log(1/delta)/(2*sample_size))+np.array([model.empirical_risk(xi) for xi in x])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78add3ab8936555f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(layout='constrained')\n",
    "# ax.set_ylim(ymin=0, ymax=20)\n",
    "ax.plot(x, y, label=\"True risk\")\n",
    "ax.plot(x, bound, label=\"Bound\")\n",
    "ax.set_xlabel('Parameter beta')\n",
    "ax.set_ylabel('Value of risk/bound')\n",
    "ax.legend()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ad41d65b9c73524"
  },
  {
   "cell_type": "markdown",
   "id": "35290cb2",
   "metadata": {},
   "source": [
    "**Exercise:** Vary over the sample size and see how the bound decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Bonus:** Even for a the large sample size ($n=1000$) the bound is not tight, i.e. the risk of the hypothesis is much smaller than the bound. Why is that?\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c25cae2bf4c8a67"
  },
  {
   "cell_type": "markdown",
   "id": "a55612f6",
   "metadata": {},
   "source": [
    "### Optional: Benchmark empirical Rademacher complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fe5e83",
   "metadata": {},
   "source": [
    "Since $\\frac{1}{K}\\sum_{k=1}^K\\mathcal{R}_k$ is only an approximate of the empirical Rademacher complexity (and is a Random variable) let us estimate its mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed425f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "result = [calculate_empirical_rademacher_complexity(\n",
    "    K=K, hypothesis_class=model,) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb19b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Estimated mean: {np.mean(result)} and estimated std: {np.std(result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f63d34",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Find some $K$ such that the standard deviation is below 10% of the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74287c86",
   "metadata": {},
   "source": [
    "Let us further look how the approximation changes with the sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%capture\n",
    "sample_sizes = [100, 200, 500, 1000]\n",
    "rademachers = []\n",
    "for sample_size in sample_sizes:\n",
    "    training_data = generate_training_data(number_samples=sample_size)\n",
    "    model = PenalizedLinearModel(\n",
    "        training_data=training_data, maximum_beta=M, q_of_q_norm=q_of_q_norm)\n",
    "    model.train(initial_guess=np.array([0]), max_iter_training=100)\n",
    "    rademachers.append(calculate_empirical_rademacher_complexity(\n",
    "        K=200, hypothesis_class=model))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d52c0786a7e9d27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67fa280",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a figure containing a single axes.\n",
    "fig, ax = plt.subplots(layout='constrained')\n",
    "ax.plot(sample_sizes, rademachers)\n",
    "ax.legend()\n",
    "ax.set_xlabel('Sample size')  # Add an x-label to the axes.\n",
    "# Add a y-label to the axes.\n",
    "ax.set_ylabel('Approximation empirical Rademacher')\n",
    "# ax.set_ylim(ymin=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d4b1a0",
   "metadata": {},
   "source": [
    "### Comparison to theoretical bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2e058e",
   "metadata": {},
   "source": [
    "In the lecture we have established a bound of the Rademacher complexity *with respect to the **hypothesis class***, i.e.\n",
    "$$\\mathcal{R}_n(\\mathcal{H})\\leq M\\sqrt{\\frac{\\mathbb{E}\\left[||Z||_2^2\\right]}{n}}.$$\n",
    "In our case, we can calculate \n",
    "$$\\mathbb{E}\\left[||Z||_2^2\\right]=\\int_{0}^1(x^2+(4x)^2)dx+var(\\epsilon)=17/3+1/3=6$$\n",
    "and, therefore, \n",
    "$$\\mathcal{R}_n(\\mathcal{H})\\leq 3\\sqrt{6/n}.$$\n",
    "\n",
    "Applying Talagrands contraction lemma, we obtain\n",
    "$$\\mathcal{R}_n(\\mathcal{L})\\leq 2\\sqrt{B}\\mathcal{R}_n(\\mathcal{H})$$\n",
    "where $B$ is the supremum of the loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e80f9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_theoretical_bound(n):\n",
    "    return 2*(sup_L)**(1/2)*M*(6/n)**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a1280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.plot(sample_sizes, [calculate_theoretical_bound(sample_size)\n",
    "        for sample_size in sample_sizes], label=\"Bound\")\n",
    "ax.set_ylim(ymin=0)\n",
    "ax.legend()\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2a251c",
   "metadata": {},
   "source": [
    "## Bound of risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a918fab",
   "metadata": {},
   "source": [
    "Recall that we are interested in a bound of the empirical loss and that the Rademacher complexity provides such a bound:\n",
    "$$R(\\hat{h})\\le R(h_0)+2\\mathcal{R}_n(\\mathcal{L})+K\\left(\\frac{ln(1/\\delta)}{2n}\\right)^{1/2}$$\n",
    "with probability at least $1-\\delta$.\n",
    "\n",
    "In our setup we can explicitly calculate the risk (since we know the underlying distribution of the samples):\n",
    "$$R(h)=\\frac{(4-\\beta)^2}{3}+1/3$$\n",
    "\n",
    "Therefore, we can calculate\n",
    "$$R(h_0)=\\frac{2}{3},\\beta_0=3.$$\n",
    "\n",
    "Above we calculated\n",
    "$$K=(15/2)^2.$$\n",
    "\n",
    "We obtain \n",
    "$$R(\\hat{h})\\le \\frac{1}{3}+2\\mathcal{R}_n(\\mathcal{L})+K^2\\left(\\frac{ln(1/\\delta)}{2n}\\right)^{1/2}.$$\n",
    "\n",
    "Let us empirically verify that bound by using the approximation of the empirical rademacher instead of $\\mathcal{R}_n(\\mathcal{L})$ (and neglecting the approximation error). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a1d077",
   "metadata": {},
   "outputs": [],
   "source": [
    "bound = calculate_risk_of_hypothesis(3)+2*calculate_empirical_rademacher_complexity(\n",
    "    K=100, hypothesis_class=model)+(15/2)**2*(np.log(1/delta)/(2*len(training_data)))**(1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfa199b",
   "metadata": {},
   "source": [
    "Now, let us convince that this is in fact an upper bound of the risk of the hypothesis calculated by our algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f74165",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The calculated bound is {bound}\")\n",
    "print(\n",
    "    f\"The calculated risk is {calculate_risk_of_hypothesis(model.trained_parameter)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c16181b",
   "metadata": {},
   "source": [
    "Let us at last look if this is also a bound of the risk of the other hypothesis.\n",
    "\n",
    "**Exercise:** What do you expect? Does the bound above really bounds all risks? If not, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ef6dd",
   "metadata": {},
   "source": [
    "Observe that in our setup $||\\beta||_q\\le M$ reads $\\beta\\in [-3,3]$.\n",
    "Lets visualize the risk of all those hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c22c09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-3, 3, 0.1)\n",
    "y = calculate_risk_of_hypothesis(x)\n",
    "\n",
    "fig, ax = plt.subplots(layout='constrained')\n",
    "ax.plot(x, y, label=\"Risk of respective hypothesis\")\n",
    "ax.plot(x, [bound for _, _ in enumerate(x)], label=\"Bound\")\n",
    "ax.set_xlabel('Parameter beta')\n",
    "ax.set_ylabel('Value of risk/bound')\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
