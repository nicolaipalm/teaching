{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0e16d38",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e9332d",
   "metadata": {},
   "source": [
    "In this notebook you will find ...\n",
    "1. A recap of the main result of the Rademacher complexity in the context of statistical learning theory\n",
    "2. A playground for applying and testing those (theoretical) results to the hypothesis class of ridge regression, \n",
    "\n",
    "The purpose of this notebook is to empirically validate all the theoretical results as well as getting a feeling how the Rademacher complexity behaves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee0e525",
   "metadata": {},
   "source": [
    "## Recap of mathematical results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09257c5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T11:37:30.888434Z",
     "start_time": "2023-05-31T11:37:30.874287Z"
    }
   },
   "source": [
    "The main result about the Rademacher complexity reads as follows:\n",
    "\n",
    "$$R(\\hat{h})\\le R(h^*)+2\\mathcal{R}_n(\\mathcal{L})+K(\\frac{ln(1/\\delta)}{2n})$$\n",
    "\n",
    "with probability at least $1-\\delta$ where $R$ denotes the risk, $\\hat{h}=\\mathcal{A}(D_n)$ is the parameter obtained by (our) algorithm, $R(h^*)$ is the minimal risk among all hypothesis, $\\mathcal{R}_n(\\mathcal{L})$ denotes the Rademacher complexity with respect to the composition of loss function with the underlying hypothesis class, $K$ is the supremum of the loss function restricted to the image of the hypothesis class. \n",
    "\n",
    "Furthermore, the empirical Rademacher complexity defined as \n",
    "$$\\hat{\\mathcal{R}}_n(\\mathcal{L})=\\mathbb{E}_{\\epsilon}(sup_{h\\in \\mathcal{H}}\\frac{1}{n}\\sum_{i=1,...,n}\\epsilon_iL\\circ h(Z_i))$$\n",
    "satisfies \n",
    "$\\mathbb{E}(\\mathcal{R}_n(\\mathcal{L}))=\\mathcal{R}_n(\\mathcal{L})$ and serves, therefore, as an approximate of the Rademacher complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3160baf1",
   "metadata": {},
   "source": [
    "## Import of required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b529f1e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:29:31.035681Z",
     "start_time": "2023-06-01T12:29:29.813025Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from modules.parametric_model import PenalizedLinearModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f13680",
   "metadata": {},
   "source": [
    "## Generating training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0df863e",
   "metadata": {},
   "source": [
    "We first generate some training data (i.e. sample the random variables $Z$) by defining\n",
    "$$Z=(X,f(X)+\\epsilon)$$\n",
    "where $X$ is uniformly distributed on the interval $[0,1]$, $\\epsilon\\sim \\mathcal{N}(0,1)$ is a white noise (independent of $X$) and $f$ is the function defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b29671e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:29:31.039730Z",
     "start_time": "2023-06-01T12:29:31.037942Z"
    }
   },
   "outputs": [],
   "source": [
    "def linear_function(x):\n",
    "    return 4*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a48870",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:29:31.042881Z",
     "start_time": "2023-06-01T12:29:31.040751Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# training data (X,f(X)) with X uniform on [0,1]\n",
    "def generate_training_data(number_samples: int):\n",
    "    return [np.array([x, linear_function(x)+np.random.normal()]) for x in np.random.uniform(0, 1, number_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03640dd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:29:31.046100Z",
     "start_time": "2023-06-01T12:29:31.044087Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_size = 10\n",
    "training_data = generate_training_data(sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70446e4e",
   "metadata": {},
   "source": [
    "## Hypothesis class: Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6c8771",
   "metadata": {},
   "source": [
    "In this notebook we deal with Ridge regression, i.e. our hypothesis class looks as follows:\n",
    "$$\\mathcal{H}=\\{x\\mapsto \\beta^Tx\\colon ||\\beta||_q\\le M\\}$$\n",
    "for some real number $M$ and positive number $q$.\n",
    "\n",
    "In the following we set \n",
    "$$q=2,M=3$$\n",
    "but you are free to change the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bc7b80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:29:31.049307Z",
     "start_time": "2023-06-01T12:29:31.047221Z"
    }
   },
   "outputs": [],
   "source": [
    "q_of_q_norm = 2\n",
    "M = 3\n",
    "# Initialize ridge regression model\n",
    "model = PenalizedLinearModel(\n",
    "    training_data=training_data, maximum_beta=M, q_of_q_norm=q_of_q_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a288c424",
   "metadata": {},
   "source": [
    "We use the ERM (expected risk minimizer) algorithm in order to obtain a hypothesis from our samples.\n",
    "This is already implemented in the train method of the above model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bca3861",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:29:31.056113Z",
     "start_time": "2023-06-01T12:29:31.050426Z"
    }
   },
   "outputs": [],
   "source": [
    "model.train(initial_guess=np.array([0]), max_iter_training=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43525a5",
   "metadata": {},
   "source": [
    "Let us visualize the training data and the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d327b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:29:31.273832Z",
     "start_time": "2023-06-01T12:29:31.057633Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize model\n",
    "x = np.array(training_data).T[0]\n",
    "y = np.array(training_data).T[1]\n",
    "\n",
    "fig, ax = plt.subplots(layout='constrained')\n",
    "ax.scatter(x, y, label=\"Training data\")\n",
    "ax.plot(x, [model(parameter=model.trained_parameter, x=xi)\n",
    "        for xi in x], label=\"Trained function\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f531539f",
   "metadata": {},
   "source": [
    "## Empirical Rademacher complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f182c",
   "metadata": {},
   "source": [
    "We wish to calculate the empirical Rademacher complexity \n",
    "$$\\hat{\\mathcal{R}}_n(\\mathcal{L})=\\mathbb{E}_{\\epsilon}(sup_{h\\in \\mathcal{H}}\\frac{1}{n}\\sum_{i=1,...,n}\\epsilon_iL\\circ h(Z_i)).$$\n",
    "However, since we are taking an expected value, we *approximate* the empirical Rademacher complexity by the following algorithm\n",
    "\n",
    "```\n",
    "    for $k=1,...,K$ \n",
    "    1. simulate Rademacher variables \n",
    "```\n",
    "$$\\epsilon_1,...,\\epsilon_n$$\n",
    "```\n",
    "    2. compute \n",
    "```\n",
    "$$\\mathcal{R}_k= max_{h\\in \\mathcal{H}}\\frac{1}{n}\\sum_{i=1}^n\\epsilon_iL\\circ h(Z_i)$$\n",
    "\n",
    "Since $$\\frac{1}{K}\\sum_{k=1}^K\\mathcal{R}_k\\overset{K\\to \\infty}\\to \\hat{\\mathcal{R}}_n$$\n",
    "almost surely, $\\frac{1}{K}\\sum_{k=1}^K\\mathcal{R}_k$ serves as an approximation of the empirical Rademacher complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cff5a9",
   "metadata": {},
   "source": [
    "**Exercise:** Implement the empirical Rademacher complexity according to the above algorithm.\n",
    "\n",
    "**Note:** The loss function (sqare loss) is alread implemented in the model (method: model.loss_function()) and takes two inputs z1, z2 where z=(x,y) (i.e. as the form of the training data). \n",
    "\n",
    "\n",
    "For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e79ef4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:29:31.278014Z",
     "start_time": "2023-06-01T12:29:31.275001Z"
    }
   },
   "outputs": [],
   "source": [
    "model.loss_function(training_data[0],training_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadc24b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:29:31.281305Z",
     "start_time": "2023-06-01T12:29:31.279402Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_empirical_rademacher_complexity(K: int,\n",
    "                                              hypothesis_class: PenalizedLinearModel,\n",
    "                                              max_iter_maximization: float = 1000):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be6e2b",
   "metadata": {},
   "source": [
    "Let us calculate the approximation of the empirical Rademacher complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830a419d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:29:31.669467Z",
     "start_time": "2023-06-01T12:29:31.282975Z"
    }
   },
   "outputs": [],
   "source": [
    "K = 100\n",
    "print(\n",
    "    f\"The approximated empirical rademacher complexity with K={K} is {calculate_empirical_rademacher_complexity(K=K,hypothesis_class=model,)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89e51c5",
   "metadata": {},
   "source": [
    "### Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9d38b2",
   "metadata": {},
   "source": [
    "Recall that the Rademacher complexity serves as a bound \n",
    "$$\\sup_{h\\in \\mathcal{H}}(R(h)-R_n(h))\\le 2\\mathcal{R}_n(\\mathcal{L})+K\\sqrt{\\frac{ln(1/\\delta)}{2n}}$$\n",
    "with probability at least $1-\\delta$.\n",
    "\n",
    "Let us verify this empirically by visualizing $R(h)-R_n(h)$ and the upper right side for all $h$.\n",
    "\n",
    "We calculate\n",
    "$$K=sup_{z,z^\\prime,l,l^\\prime}|l(z)-l(z^\\prime)|=sup_{z,z^\\prime,l,l^\\prime}(4-\\beta)^2|x^2-x^{\\prime 2}|=7^2.$$\n",
    "\n",
    "In our setup we can explicitely calculate the risk (since we know the underlying distribution of the samples):\n",
    "$$R(h)=\\int_{0}^1(4x-\\beta x)^2dx+var(\\epsilon)=\\frac{(4-\\beta)^2}{3}+1$$\n",
    "\n",
    "Observe that in our setup $||\\beta||_q\\le M$ reads $\\beta\\in [-3,3]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c4391e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:29:31.684552Z",
     "start_time": "2023-06-01T12:29:31.684544Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_risk_of_hypothesis(beta):\n",
    "    return ((4-beta)**2)/3+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e4c65e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:29:31.685306Z",
     "start_time": "2023-06-01T12:29:31.685298Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "training_data = generate_training_data(sample_size)\n",
    "model = PenalizedLinearModel(\n",
    "    training_data=training_data, maximum_beta=M, q_of_q_norm=q_of_q_norm)\n",
    "model.train(initial_guess=0)\n",
    "delta = 0.05\n",
    "rademacher = calculate_empirical_rademacher_complexity(K=100,hypothesis_class=model)\n",
    "bound = 2*rademacher+7**2*np.sqrt(np.log(1/delta)/(2*sample_size))\n",
    "\n",
    "x = np.arange(-3, 3, 0.1)\n",
    "y = calculate_risk_of_hypothesis(x)-np.array([model.empirical_risk(xi) for xi in x])\n",
    "\n",
    "# Create a figure containing a single axes.\n",
    "fig, ax = plt.subplots(layout='constrained')\n",
    "ax.plot(x, y, label=\"Risk minus empirical risk of respective hypothesis\")\n",
    "ax.plot(x, [bound for _, _ in enumerate(x)], label=\"Bound\")\n",
    "ax.set_xlabel('Parameter beta')\n",
    "ax.set_ylabel('Value of risk/bound')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35290cb2",
   "metadata": {},
   "source": [
    "**Exercise:** Vary over the sample size and see how the bound decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55612f6",
   "metadata": {},
   "source": [
    "### Optional: Benchmark empirical Rademacher complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fe5e83",
   "metadata": {},
   "source": [
    "Since $\\frac{1}{K}\\sum_{k=1}^K\\mathcal{R}_k$ is only an approximate of the empirical Rademacher complexity (and is a Random variable) let us estimate its mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed425f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:29:31.686249Z",
     "start_time": "2023-06-01T12:29:31.686241Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = [calculate_empirical_rademacher_complexity(\n",
    "    K=K, hypothesis_class=model,) for _ in range(40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb19b8e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:29:31.687431Z",
     "start_time": "2023-06-01T12:29:31.687423Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Estimated mean: {np.mean(result)} and estimated std: {np.std(result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f63d34",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Find some $K$ such that the standard deviation is below 10% of the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74287c86",
   "metadata": {},
   "source": [
    "Let us further look how the approximation behaves with respect to the sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67fa280",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:29:31.688634Z",
     "start_time": "2023-06-01T12:29:31.688614Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize model\n",
    "sample_sizes = [100, 200, 500, 1000]\n",
    "rademachers = []\n",
    "for sample_size in sample_sizes:\n",
    "    training_data = generate_training_data(number_samples=sample_size)\n",
    "    model = PenalizedLinearModel(\n",
    "        training_data=training_data, maximum_beta=M, q_of_q_norm=q_of_q_norm)\n",
    "    model.train(initial_guess=np.array([0]), max_iter_training=100)\n",
    "    rademachers.append(calculate_empirical_rademacher_complexity(\n",
    "        K=200, hypothesis_class=model))\n",
    "\n",
    "# Create a figure containing a single axes.\n",
    "fig, ax = plt.subplots(layout='constrained')\n",
    "ax.plot(sample_sizes, rademachers)\n",
    "ax.legend()\n",
    "ax.set_xlabel('Sample size')  # Add an x-label to the axes.\n",
    "# Add a y-label to the axes.\n",
    "ax.set_ylabel('Approximation empirical Rademacher')\n",
    "#ax.set_ylim(ymin=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d4b1a0",
   "metadata": {},
   "source": [
    "### Comparison to theoretical bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2e058e",
   "metadata": {},
   "source": [
    "In the lecture we have established a bound of the Rademacher complexity *with respect to the **hypothesis class***, i.e.\n",
    "$$\\mathcal{R}_n(\\mathcal{H})\\leq M\\sqrt{\\frac{\\mathbb{E}(||Z||_2^2)}{n}}.$$\n",
    "In our case, we can calculate \n",
    "$$\\mathbb{E}(||Z||_2^2)=\\int_{0}^1(x^2+(4x)^2)dx+var(\\epsilon)=\\frac{17}{3}+1=\\frac{20}{3}$$\n",
    "and, therefore, \n",
    "$$\\mathcal{R}_n(\\mathcal{H})\\leq 3\\sqrt{\\frac{17}{3n}}.$$\n",
    "\n",
    "Applying Talagrands contraction lemma, we obtain\n",
    "$$\\mathcal{R}_n(\\mathcal{L})\\leq 2B\\mathcal{R}_n(\\mathcal{H})$$\n",
    "where $B$ is the maximum of all $x$, i.e. $B=1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e80f9f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:29:31.689803Z",
     "start_time": "2023-06-01T12:29:31.689788Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_theoretical_bound(n):\n",
    "    return 2*M*(20/(3*n))**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a1280b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:29:31.690746Z",
     "start_time": "2023-06-01T12:29:31.690738Z"
    }
   },
   "outputs": [],
   "source": [
    "ax.plot(sample_sizes, [calculate_theoretical_bound(sample_size)\n",
    "        for sample_size in sample_sizes], label=\"Bound\")\n",
    "#ax.set_ylim(ymin=0)\n",
    "ax.legend()\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2a251c",
   "metadata": {},
   "source": [
    "## Bound of risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a918fab",
   "metadata": {},
   "source": [
    "Recall that we are interested in a bound of the empirical loss and that the Rademacher complexity provides such a bound:\n",
    "$$R(\\hat{h})\\le R(h^*)+2\\mathcal{R}_n(\\mathcal{L})+K(\\frac{ln(1/\\delta)}{2n})$$\n",
    "with probability at least $1-\\delta$.\n",
    "\n",
    "In our setup we can explicitely calculate the risk (since we know the underlying distribution of the samples):\n",
    "$$R(h)=\\frac{(4-\\beta)^2}{3}+1$$\n",
    "\n",
    "Therefore, we can calculate\n",
    "$$R(h^*)=\\frac{1}{3},\\beta^*=3.$$\n",
    "\n",
    "Above we calculated\n",
    "$$K=7^2.$$\n",
    "\n",
    "We obtain \n",
    "$$R(\\hat{h})\\le \\frac{1}{3}+2\\mathcal{R}_n(\\mathcal{L})+7^2(\\frac{ln(1/\\delta)}{2n}).$$\n",
    "\n",
    "Let us empirically varify that bound by using the approximation of the empirical rademacher instead of $\\mathcal{R}_n(\\mathcal{L})$ (and neglecting the approximation error). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a1d077",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:29:31.692123Z",
     "start_time": "2023-06-01T12:29:31.692115Z"
    }
   },
   "outputs": [],
   "source": [
    "bound = calculate_risk_of_hypothesis(3)+2*calculate_empirical_rademacher_complexity(\n",
    "    K=100, hypothesis_class=model)+7**2*(np.log(1/delta)/(2*len(training_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfa199b",
   "metadata": {},
   "source": [
    "Now, let us convince that this is in fact an upper bound of the risk of the hypothesis calculated by our algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f74165",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:29:31.693215Z",
     "start_time": "2023-06-01T12:29:31.693204Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"The calculated bound is {bound}\")\n",
    "print(\n",
    "    f\"The calculated risk is {calculate_risk_of_hypothesis(model.trained_parameter)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c16181b",
   "metadata": {},
   "source": [
    "Let us at last look if this is also a bound of the risk of the other hypothesis.\n",
    "\n",
    "**Exercise:** What do you expect? Does the bound above really bounds all risks? If not, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5ef6dd",
   "metadata": {},
   "source": [
    "Observe that in our setup $||\\beta||_q\\le M$ reads $\\beta\\in [-3,3]$.\n",
    "Lets visualize the risk of all those hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c22c09e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T12:29:31.694148Z",
     "start_time": "2023-06-01T12:29:31.694137Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.arange(-3, 3, 0.1)\n",
    "y = calculate_risk_of_hypothesis(x)\n",
    "\n",
    "fig, ax = plt.subplots(layout='constrained')\n",
    "ax.plot(x, y, label=\"Risk of respective hypothesis\")\n",
    "ax.plot(x, [bound for _, _ in enumerate(x)], label=\"Bound\")\n",
    "ax.set_xlabel('Parameter beta')\n",
    "ax.set_ylabel('Value of risk/bound')\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
